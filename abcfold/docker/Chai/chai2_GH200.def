Bootstrap: docker
From: nvidia/cuda:12.6.3-devel-ubuntu24.04
%post
    mkdir -p /host/nccl

    # Install system packages including kalign for templates
    DEBIAN_FRONTEND=noninteractive \
    apt-get update --quiet \
    && apt-get install --yes --quiet --allow-change-held-packages \
        python3.12 python3-pip python3.12-venv python3.12-dev \
        git wget gcc g++ make zlib1g-dev \
        libnccl2 libnccl-dev \
        kalign \
        numactl

    # Create Python virtual environment
    python3 -m venv /chai_venv
    /chai_venv/bin/pip3 install --no-cache-dir --upgrade pip setuptools wheel

    # Install PyTorch for ARM with CUDA 12
    /chai_venv/bin/pip3 install --no-cache-dir \
        torch==2.4.0 \
        --index-url https://download.pytorch.org/whl/cu124

    # Install JAX 0.4.30 with CUDA 12 support (compatible with numpy 1.26)
    /chai_venv/bin/pip3 install --no-cache-dir \
        jax[cuda12]==0.4.30 \
        -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

    # Install Chai-1 and dependencies
    /chai_venv/bin/pip3 install --no-cache-dir \
        chai_lab==0.6.1 \
        antipickle typer jaxtyping beartype pandera matplotlib

    mkdir -p /chai_venv/lib/python3.10/site-packages/downloads
    cd /chai_venv/lib/python3.10/site-packages/downloads

    wget --tries=3 --timeout=30 \
        https://chaiassets.com/chai1-inference-depencencies/conformers_v1.apkl \
        https://chaiassets.com/chai1-inference-depencencies/leaving_atoms_v1.apkl \
        || echo "Warning: Cache download failed, will download at runtime"

    # Download ESM model (large file ~3GB)
    wget --tries=3 --timeout=60 --continue \
        -P esm/ \
        https://chaiassets.com/chai1-inference-depencencies/esm2/traced_sdpa_esm2_t36_3B_UR50D_fp16.pt \
        || echo "Warning: ESM model download failed, will download at runtime"

    # Cleanup
    apt-get clean && rm -rf /var/lib/apt/lists/*
%environment
    export PATH="/chai_venv/bin:$PATH"

    # GH200 Hopper optimizations
    export XLA_FLAGS="--xla_gpu_enable_triton_gemm=false"
    export XLA_PYTHON_CLIENT_PREALLOCATE=false
    export XLA_CLIENT_MEM_FRACTION=0.90
    export TF_GPU_THREAD_MODE=gpu_private
    export TF_GPU_THREAD_COUNT=8

    # NCCL for Grace-Hopper NVLink-C2C
    export NCCL_NVLS_ENABLE=1
    export NCCL_NET_GDR_LEVEL=5
    export CUDA_DEVICE_ORDER=PCI_BUS_ID
%runscript
    exec /chai_venv/bin/python3 -m chai_lab.chai1 "$@"
%labels
    Author Chai_Team
    Version 0.6.1_GH200
    Architecture arm64
